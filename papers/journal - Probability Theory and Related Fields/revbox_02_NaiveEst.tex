\section {Multinomial Distribution and Naive Entropy Estimates}

A multinomial distribution [\ref{bib:Multinomial Dirichlet}] model plays the main role in investigating of point set structures. Let $n \in \mathbb{N}$ be a number of distinguished events. Let $p_{j} > 0$ be a probability of the $j^{\text{th}}$ event for $j = 1,...,n$ satisfying $ \sum_{j=1}^{n} p_{j} =1$. Then the random variable $j$ has a multinomial distribution $\text{Mul}(p_{1},...,p_{n})$. After realization of multinomial distribution sample of size $N \in \mathbb{N}$, we can count the events and obtain $N_{j} \in \mathbb{N}_{0}$ as the number of $j^{\text{th}}$ event occurrences for $j=1,...,n$ satisfying $\sum_{j=1}^{n} N_{j} = N$. Therefore, we define the number of various events in a sample as $K = \sum_{N_{j}>0} 1 \le \text{min}(n,N)$. Revising Hartley [\ref{bib:Renyi}] and Shannon [\ref{bib:Renyi}] entropy definitions
\begin{equation} 
\label{eq:hnula}
H_{0}=\ln{n},
\end{equation} 
\begin{equation} 
\label{eq:hjedna}
H_{1}=-\sum_{j=1}^{n} p_{j}\ln{p_{j}},
\end{equation}   
we can perform a direct but naive estimation of them as
\begin{equation} 
\label{eq:hnulaap}
\hat{H}_{0,\mbox{\scriptsize{naive}}}=\ln{K},
\end{equation} 
\begin{equation} 
\label{eq:hjednaap}
\hat{H}_{1,\mbox{\scriptsize{naive}}}=-\sum_{N_{j}>0} \frac{N_{j}}{N}\ln{\frac{N_{j}}{N}}.
\end{equation}   
The main disadvantage of the naive estimates is their biasness. The random variable $K \in \{ 1,...,n \} $ is capped by $n$, which causes $\text{E}\hat{H}_{0,\mbox{\scriptsize{naive}}} = \text{E}\ln{K} < \text{E}\ln{n} = \ln{n} = H_{0}$. Hence, the naive estimate of Hartley entropy $\hat{H}_{0,\mbox{\scriptsize{naive}}}$ is negatively biased. On the other hand, the traditional Box Counting Technique is based on this estimate. There we plot the logarithm of the covering element number $C(a) \in \mathbb{N}$ against the logarithm of the covering element size $a > 0$ and then estimate their dependency in the linear form $\ln{C(a)} = A_{0} - \hat{D}_{0,\mbox{\scriptsize{naive}}}\ln{a}$. Recognizing equivalence $C(a) = K$ leads to $\ln{C(a)} = \ln{K} = \hat{H}_{0,\mbox{\scriptsize{naive}}}$ and then $\hat{H}_{0,\mbox{\scriptsize{naive}}} = A_{0} - \hat{D}_{0,\mbox{\scriptsize{naive}}}\ln{a}$. Defining $\hat{D}_{0,\mbox{\scriptsize{naive}}}$ as an estimate of capacity dimension and recognizing the occurrence of $\hat{H}_{0,\mbox{\scriptsize{naive}}}$ in the Box Counting procedure [\ref{bib:Box counting}], we are not surprised to be victims of the bias of Hartley entropy estimate.\\ 
\\*
A similar situation is the case of Shannon entropy estimation. There are several approaches how to decrease the bias of $\hat{H}_{1,\mbox{\scriptsize{naive}}}$ to be closer to a theoretical value of Shannon entropy $H_{1}$. Miller [\ref{bib:Harris}] modified the naive estimate $\hat{H}_{1,\mbox{\scriptsize{naive}}}$ using a first-order Taylor expansion resulting in
\begin{equation}
\label{eq:miller}
\hat{H}_{1,\mbox{\scriptsize{M}}}=\hat{H}_{1,\mbox{\scriptsize{naive}}} + \frac{K-1}{2N}.
\end{equation}
Lately, Harris [\ref{bib:Harris}] improved the formula to
\begin{equation}
\label{eq:harris1h}
\hat{H}_{1,\mbox{\scriptsize{H}}}=\hat{H}_{1,\mbox{\scriptsize{naive}}} + \frac{K-1}{2N} - \frac{1}{12N^2} \left( 1 - \sum_{p_{j}>0}\frac{1}{p_{j}} \right).
\end{equation}
Finally, we can estimate the capacity and information dimensions according to relation
\begin{equation} 
\label{eq:hjednaest}
\hat{H}_{d}=A_{d} - \hat{D}_{d} \ln{a},
\end{equation} 
where $\hat{H}_{d}$ is any estimate of $H_{d}$. Therefore, we can also estimate Hausdorff dimension $D_{\mbox{\scriptsize{H}}}$ using inequalities $D_{1} \le D_{\mbox{\scriptsize{H}}} \le D_{0}$ under the assumption that $\hat{D}_{1} \le D_{\mbox{\scriptsize{H}}} \le \hat{D}_{0}$ for any ``good'' estimates $\hat{D}_{0}$, $\hat{D}_{1}$ of capacity and information dimensions, respectively. The next section is oriented to Bayesian estimation of $H_{0}$ and $H_{1}$, which are essential for evaluating $\hat{D}_{0}$ and $\hat{D}_{1}$.