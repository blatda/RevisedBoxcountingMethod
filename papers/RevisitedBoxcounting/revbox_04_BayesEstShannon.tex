\section {Bayesian Estimation of Shannon Entropy}
In the case when the number of events $n$ is known, we can perform Bayesian estimation of Shannon entropy as
\begin{equation} 
\label{eq:hjednan}
\hat{H}_{1,\mbox{\scriptsize{n}}} = \text{E}H_{1}(K=n) = -\sum_{j=1}^{n} \left( \frac{N_{j}+1}{N+n} \left( \psi(N_{j}+2) - \psi(N+n+1) \right) \right)
\end{equation}
where $\psi$ is digamma function. But when the number of events $n$ is unkonown, we can use $K$ as lower estimate of $n$ and perform final Bayesian estimation as
\begin{equation} 
\label{eq:hjednab}
H_{1,\mbox{\scriptsize{Bayes}}} = \sum_{n=K}^{\infty}{\text{p}\left(n \: \middle| \:K,N \right)H_{1,\mbox{\scriptsize{n}}}}
\end{equation}
which is also convergent sum for $N \ge K+2$. \\
\\*
Substituing $n=K+j$ we obtain adequating formula.
\begin{equation} 
\label{eq:hjbb}
\hat{H}_{1,\mbox{\scriptsize{Bayes}}} = \frac{\sum_{j=0}^{\infty}b_{j}H_{1,K+j}}{\sum_{j=0}^{\infty}b_{j}}
\end{equation}
Asymptotic expansion of (\ref{eq:hjbb}) unfortunately depends on individual frequences $N_{j}$.