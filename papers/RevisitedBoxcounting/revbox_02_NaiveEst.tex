\section {Multinomial Distribution and Naive Entropy Estimates}

Multinomial distrubution model plays main role in investigation of point set structures. Let $n \in \mathbb{N}$ be number of distinguish events. Let $p_{j} > 0$ be probability of $j^{\text{th}}$ event for $j = 1,...,n$ satisfying $ \sum_{j=1}^{n} p_{j} =1$. Then random variable $j$ has multinomial distribution $\text{Mul}(p_{1},...,p_{n})$. After realization of multinomial distribution sample of size $N \in \mathbb{N}$, we can count the events and obtain $N_{j} \in \mathbb{N}_{0}$ as number of $j^{\text{th}}$ event occurences for $j=1,...,n$ satisfying $\sum_{j=1}^{n} N_{j} = N$. Therefore, we define number of various events in sample as $K = \sum_{N_{j}>0} 1 \le \text{min}(n,N)$. Remembering Hartley and Shannon entropy definition as
\begin{equation} 
\label{eq:hnula}
H_{0}=\ln{n},
\end{equation} 
\begin{equation} 
\label{eq:hjedna}
H_{1}=-\sum_{j=1}^{n} p_{j}\ln{p_{j}},
\end{equation}   
we can perform direct but naive estimation of them as
\begin{equation} 
\label{eq:hnulaap}
\hat{H}_{0,\mbox{\scriptsize{naive}}}=\ln{K},
\end{equation} 
\begin{equation} 
\label{eq:hjednaap}
\hat{H}_{1,\mbox{\scriptsize{naive}}}=-\sum_{N_{j}>0} \frac{N_{j}}{N}\ln{\frac{N_{j}}{N}}.
\end{equation}   
The main disadvantage of naive estimates is their biasness. Random variable $K= \{ 1,...,n \} $ is upper constrained by $n$, then $\text{E}\hat{H}_{0,\mbox{\scriptsize{naive}}} = \text{E}\ln{K} < \text{E}\ln{n} = \ln{n} = H_{0}$. Therefore, naive estimate of Hartley entropy $\hat{H}_{0,\mbox{\scriptsize{naive}}}$ is negative biased. On the other hand, traditional Box Counting Technique is based on this estimate because we plot logarithm of covering element number $C(a) \in \mathbb{N}$ against logarithm of covering element size $a > 0$ and then estimate their dependency in linear form $\ln{C(a)} = A_{0} - \hat{D}_{0,\mbox{\scriptsize{naive}}}\ln{a}$. Recognizing equivalence $C(a) = K$, we obtain $\ln{C(a)} = \ln{K} = \hat{H}_{0,\mbox{\scriptsize{naive}}}$ and then $\hat{H}_{0,\mbox{\scriptsize{naive}}} = A_{0} - \hat{D}_{0,\mbox{\scriptsize{naive}}}\ln{a}$. Defining $\hat{D}_{0,\mbox{\scriptsize{naive}}}$ as estimate of capacity dimension and recognizing the occurence of $\hat{H}_{0,\mbox{\scriptsize{naive}}}$ in Box Counting procedure, we are not suprised to be victims of the bias of Hartley entropy estimate.\\ 
\\*
Similar situation is the case of Shannon entropy estimation. There are several approaches how to decrease the bias of $\hat{H}_{1,\mbox{\scriptsize{naive}}}$ to be closer to Shannon entropy $H_{1}$. Miller [\ref{Harris}] modified naive estimate $\hat{H}_{1,\mbox{\scriptsize{naive}}}$ using first order Taylor expansion, which produces
\begin{equation}
\label{eq:miller}
\hat{H}_{1,\mbox{\scriptsize{M}}}=\hat{H}_{1,\mbox{\scriptsize{naive}}} + \frac{K-1}{2N}.
\end{equation}
Lately, Harris [\ref{Harris}] improved the formula to
\begin{equation}
\label{eq:harris1h}
\hat{H}_{1,\mbox{\scriptsize{H}}}=\hat{H}_{1,\mbox{\scriptsize{naive}}} + \frac{K-1}{2N} - \frac{1}{12N^2} \left( 1 - \sum_{p_{j}>0}\frac{1}{p_{j}} \right)
\end{equation}
Finally, we can estimate capacity and information dimension according to relation
\begin{equation} 
\label{eq:hjednaest}
\hat{H}_{d}=A_{d} - \hat{D}_{d} \ln{a}
\end{equation} 
where $\hat{H}_{d}$ is any estimate of $H_{d}$. Therefore, we can also estimate Hausdorff dimension $D_{\mbox{\scriptsize{H}}}$ using inequalities $D_{1} \le D_{\mbox{\scriptsize{H}}} \le D_{0}$ and then also supposing $\hat{D}_{1} \le D_{\mbox{\scriptsize{H}}} \le \hat{D}_{0}$ for any ``good'' estimates $\hat{D}_{0}$, $\hat{D}_{1}$ of capacity and information dimensions. Next section is oriented to Bayesian estimation of $H_{0}$, $H_{1}$ for $\hat{D}_{0}$ and $\hat{D}_{1}$ evaluations.